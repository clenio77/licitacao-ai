#!/usr/bin/env python3
"""
Script para executar web scraping e popular a base de conhecimento.
Pode ser executado manualmente ou agendado via cron.
"""

import asyncio
import sys
import os
from datetime import datetime

# Adicionar o diretÃ³rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from web_scraping.gov_procurement_scraper import GovProcurementScraper

async def main():
    """FunÃ§Ã£o principal para executar o scraping"""
    print("ğŸš€ Iniciando coleta de dados de licitaÃ§Ãµes bem-sucedidas...")
    print(f"â° Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Categorias para coletar
    categorias_prioritarias = [
        'serviÃ§os de limpeza',
        'equipamentos de informÃ¡tica',
        'material de escritÃ³rio',
        'serviÃ§os de seguranÃ§a',
        'serviÃ§os de manutenÃ§Ã£o',
        'veÃ­culos',
        'combustÃ­vel',
        'serviÃ§os de telecomunicaÃ§Ãµes',
        'mÃ³veis e utensÃ­lios',
        'material de construÃ§Ã£o'
    ]
    
    try:
        # Criar instÃ¢ncia do scraper
        scraper = GovProcurementScraper()
        
        # Executar scraping
        print(f"ğŸ” Coletando dados para {len(categorias_prioritarias)} categorias...")
        licitacoes = await scraper.scrape_all_sites(categorias_prioritarias)
        
        if licitacoes:
            # Salvar na base de conhecimento
            filepath = await scraper.save_to_knowledge_base(licitacoes)
            
            print(f"âœ… Scraping concluÃ­do com sucesso!")
            print(f"ğŸ“Š Total de licitaÃ§Ãµes coletadas: {len(licitacoes)}")
            print(f"ğŸ’¾ Dados salvos em: {filepath}")
            
            # EstatÃ­sticas por categoria
            categorias_count = {}
            for lic in licitacoes:
                cat = lic.categoria
                categorias_count[cat] = categorias_count.get(cat, 0) + 1
            
            print("\nğŸ“ˆ DistribuiÃ§Ã£o por categoria:")
            for categoria, count in categorias_count.items():
                print(f"  - {categoria}: {count} licitaÃ§Ãµes")
            
            # EstatÃ­sticas por site
            sites_count = {}
            for lic in licitacoes:
                site = lic.site_origem
                sites_count[site] = sites_count.get(site, 0) + 1
            
            print("\nğŸŒ DistribuiÃ§Ã£o por site:")
            for site, count in sites_count.items():
                print(f"  - {site}: {count} licitaÃ§Ãµes")
                
        else:
            print("âš ï¸ Nenhuma licitaÃ§Ã£o foi coletada")
            print("ğŸ’¡ Verifique a conectividade e disponibilidade dos sites")
            
    except Exception as e:
        print(f"âŒ Erro durante o scraping: {str(e)}")
        sys.exit(1)
    
    print(f"\nğŸ Processo finalizado em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    # Verificar se o diretÃ³rio data existe
    data_dir = "data"
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print(f"ğŸ“ DiretÃ³rio {data_dir} criado")
    
    # Executar scraping
    asyncio.run(main())
